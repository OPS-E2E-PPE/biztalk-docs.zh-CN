---
title: HYPER-V 上的系统资源成本 |Microsoft 文档
ms.custom: ''
ms.date: 06/08/2017
ms.prod: biztalk-server
ms.reviewer: ''
ms.suite: ''
ms.tgt_pltfrm: ''
ms.topic: article
ms.assetid: 9f25a76c-1c41-41c0-b28d-d7473dbe1cd1
caps.latest.revision: ''
author: MandiOhlinger
ms.author: mandia
manager: anneta
ms.openlocfilehash: 491c71a446829ddddfc4d7c55053b94dcf7fc9d1
ms.sourcegitcommit: 8418b1a8f38b7f56979cd6e203f0b591e2f40fe1
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 03/28/2018
---
# <a name="system-resource-costs-on-hyper-v"></a><span data-ttu-id="b886e-102">HYPER-V 上的系统资源成本</span><span class="sxs-lookup"><span data-stu-id="b886e-102">System Resource Costs on Hyper-V</span></span>
## <a name="system-resource-costs-associated-with-running-a-guest-operating-system-on-hyper-v"></a><span data-ttu-id="b886e-103">与 HYPER-V 上运行来宾操作系统的系统资源成本</span><span class="sxs-lookup"><span data-stu-id="b886e-103">System Resource Costs Associated with Running a Guest Operating System on Hyper-V</span></span>  
 <span data-ttu-id="b886e-104">与任何服务器虚拟化软件一样，没有一定量的与运行在 HYPER-V 上运行的来宾操作系统的支持所需的虚拟化代码关联的开销。</span><span class="sxs-lookup"><span data-stu-id="b886e-104">As with any server virtualization software, there is a certain amount of overhead associated with running the virtualization code required to support guest operating systems running on Hyper-V.</span></span> <span data-ttu-id="b886e-105">以下列表总结了 HYPER-V 虚拟机上运行来宾操作系统时，与特定资源关联的开销：</span><span class="sxs-lookup"><span data-stu-id="b886e-105">The following list summarizes the overhead associated with specific resources when running guest operating systems on Hyper-V virtual machines:</span></span>  
  
### <a name="cpu-overhead"></a><span data-ttu-id="b886e-106">CPU 开销</span><span class="sxs-lookup"><span data-stu-id="b886e-106">CPU Overhead</span></span>  
 <span data-ttu-id="b886e-107">CPU 与 HYPER-V 虚拟机中运行来宾操作系统的系统开销关联找到 9 和 12%之间的范围。</span><span class="sxs-lookup"><span data-stu-id="b886e-107">The CPU overhead associated with running a guest operating system in a Hyper-V virtual machine was found to range between 9 and 12%.</span></span>  <span data-ttu-id="b886e-108">例如，来宾操作系统运行的 HYPER-V 虚拟机上通常具有可用于在物理硬件上运行等效的操作系统的 CPU 资源的可用 88 91%。</span><span class="sxs-lookup"><span data-stu-id="b886e-108">For example, a guest operating system running on a Hyper-V virtual machine typically had available 88-91% of the CPU resources available to an equivalent operating system running on physical hardware.</span></span>  
  
### <a name="memory-overhead"></a><span data-ttu-id="b886e-109">内存开销</span><span class="sxs-lookup"><span data-stu-id="b886e-109">Memory Overhead</span></span>  
 <span data-ttu-id="b886e-110">对于 HYPER-V 主机计算机，与 HYPER-V 虚拟机上运行来宾操作系统相关的内存开销观察到要用于虚拟机监控程序，大约 300 MB 加上 32 MB; 在第一个 GB 的 RAM 分配给每个虚拟机，以及另一个 8 MB为每个其他 GB 的 RAM 分配给每个虚拟机。</span><span class="sxs-lookup"><span data-stu-id="b886e-110">For the Hyper-V host computer, the memory cost associated with running a guest operating system on a Hyper-V virtual machine was observed to be approximately 300 MB for the hypervisor, plus 32 MB for the first GB of RAM allocated to each virtual machine, plus another 8 MB for every additional GB of RAM allocated to each virtual machine.</span></span> <span data-ttu-id="b886e-111">有关分配到 HYPER-V 虚拟机上运行的来宾操作系统的内存的详细信息，请参阅中的"优化内存性能"部分[清单： HYPER-V 上优化性能](~/technical-guides/checklist-optimizing-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="b886e-111">For more information about allocating memory to guest operating systems running on a Hyper-V virtual machine, see the “Optimizing Memory Performance” section in [Checklist: Optimizing Performance on Hyper-V](~/technical-guides/checklist-optimizing-performance-on-hyper-v.md).</span></span>  
  
### <a name="network-overhead"></a><span data-ttu-id="b886e-112">网络开销</span><span class="sxs-lookup"><span data-stu-id="b886e-112">Network Overhead</span></span>  
 <span data-ttu-id="b886e-113">直接在 HYPER-V 虚拟机中运行来宾操作系统是由引起观察到的网络延迟是小于 1 ms 和来宾操作系统通常维护的网络输出队列长度小于 1。</span><span class="sxs-lookup"><span data-stu-id="b886e-113">Network latency directly attributable to running a guest operating system in a Hyper-V virtual machine was observed to be less than 1 ms and the guest operating system typically maintained a network output queue length of less than one.</span></span> <span data-ttu-id="b886e-114">有关测量网络输出队列长度的详细信息，请参阅中的"测量网络性能"部分[清单： HYPER-V 上的测量性能](../technical-guides/checklist-measuring-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="b886e-114">For more information about measuring the network output queue length, see the “Measuring Network Performance” section in [Checklist: Measuring Performance on Hyper-V](../technical-guides/checklist-measuring-performance-on-hyper-v.md).</span></span>  
  
### <a name="disk-overhead"></a><span data-ttu-id="b886e-115">磁盘开销</span><span class="sxs-lookup"><span data-stu-id="b886e-115">Disk Overhead</span></span>  
 <span data-ttu-id="b886e-116">当在 HYPER-V 中使用的传递磁盘功能，到 6 和 8%之间的范围未找到磁盘与 HYPER-V 虚拟机中运行来宾操作系统相关的 I/O 开销。</span><span class="sxs-lookup"><span data-stu-id="b886e-116">When using the passthrough disk feature in Hyper-V, disk I/O overhead associated with running a guest operating system in a Hyper-V virtual machine was found to range between 6 and 8 %.</span></span> <span data-ttu-id="b886e-117">例如，通常在 HYPER-V 上运行来宾操作系统具有磁盘 I/O 供等效操作系统在由基准测试工具 IOMeter 的开放源磁盘性能的物理硬件上运行的可用 92 94%。</span><span class="sxs-lookup"><span data-stu-id="b886e-117">For example, a guest operating system running on Hyper-V typically had available 92-94% of the disk I/O available to an equivalent operating system running on physical hardware as measured by the open source disk performance benchmarking tool IOMeter.</span></span>  
  
 <span data-ttu-id="b886e-118">有关测量磁盘延迟 HYPER-V 主机或来宾操作系统上使用性能监视器的信息，请参阅中的"磁盘 I/O 性能测量"一节[清单： HYPER-V 上的测量性能](../technical-guides/checklist-measuring-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="b886e-118">For information about measuring disk latency on a Hyper-V host or guest operating system using Performance Monitor, see the “Measuring Disk I/O Performance” section in [Checklist: Measuring Performance on Hyper-V](../technical-guides/checklist-measuring-performance-on-hyper-v.md).</span></span>  
  
 <span data-ttu-id="b886e-119">本部分的其余部分提供有关 BizTalk Server 磁盘性能的背景信息、 介绍的测试配置参数使用，并提供获得的测试结果的摘要。</span><span class="sxs-lookup"><span data-stu-id="b886e-119">The remainder of this section provides background information on BizTalk Server disk performance, describes the test configuration parameters used, and provides a summary of test results obtained.</span></span>  
  
#### <a name="disk-performance-when-running-a-biztalk-server-solution-on-hyper-v"></a><span data-ttu-id="b886e-120">在 HYPER-V 上运行 BizTalk Server 解决方案时的磁盘性能</span><span class="sxs-lookup"><span data-stu-id="b886e-120">Disk Performance When Running a BizTalk Server Solution on Hyper-V</span></span>  
 <span data-ttu-id="b886e-121">BizTalk Server 是极数据库密集型应用程序可能需要最多 13 SQL Server 数据库的创建。</span><span class="sxs-lookup"><span data-stu-id="b886e-121">BizTalk Server is an extremely database intensive application that may require the creation of up to 13 databases in SQL Server.</span></span> <span data-ttu-id="b886e-122">BizTalk Server 仍然数据存储到磁盘存在很好的频率和此外，MSDTC 事务的上下文中执行此。</span><span class="sxs-lookup"><span data-stu-id="b886e-122">BizTalk Server persists data to disk with great frequency and furthermore, does so within the context of an MSDTC transaction.</span></span> <span data-ttu-id="b886e-123">因此，数据库性能至关重要的 BizTalk Server 中的任何解决方案的总体性能。</span><span class="sxs-lookup"><span data-stu-id="b886e-123">Therefore, database performance is paramount to the overall performance of any BizTalk Server solution.</span></span> <span data-ttu-id="b886e-124">HYPER-V 提供了合成 SCSI 控制器，这两者都提供显著的性能改进了对使用模拟的 IDE 设备如 IDE 筛选器驱动程序提供使用 Virtual Server 2005。</span><span class="sxs-lookup"><span data-stu-id="b886e-124">Hyper-V provides a synthetic SCSI controller and an IDE filter driver which both provide significant performance benefits over using an emulated IDE device such as is provided with Virtual Server 2005.</span></span>  
  
 <span data-ttu-id="b886e-125">配置为使用 SCSI 控制器的数据卷的磁盘。</span><span class="sxs-lookup"><span data-stu-id="b886e-125">Configure disks for data volumes using the SCSI controller.</span></span> <span data-ttu-id="b886e-126">这可以保证因为而模拟的 IDE 控制器可用的而无需安装 HYPER-V 集成服务已安装 HYPER-V 集成服务情况下，才会安装 SCSI 控制器，安装了集成服务。</span><span class="sxs-lookup"><span data-stu-id="b886e-126">This will guarantee that the integration services are installed because the SCSI controller can only be installed if Hyper-V integration services are installed whereas the emulated IDE controller is available without installing Hyper-V integration services.</span></span> <span data-ttu-id="b886e-127">使用 SCSI 控制器或使用 integration services 提供的 IDE 筛选器驱动程序执行的磁盘 I/O 是明显优于磁盘输入/输出性能提供与模拟的 IDE 控制器。</span><span class="sxs-lookup"><span data-stu-id="b886e-127">Disk I/O performed using either the SCSI controller or the IDE filter driver provided with integration services is significantly better than disk I/O performance provided with the emulated IDE controller.</span></span> <span data-ttu-id="b886e-128">因此，若要确保最佳磁盘的 HYPER-V 虚拟化环境中的数据文件的 I/O 性能，在主机和来宾操作系统上安装集成服务，并且使用合成 SCSI 控制器配置数据卷的磁盘。</span><span class="sxs-lookup"><span data-stu-id="b886e-128">Therefore, to ensure optimal disk I/O performance for the data files in a Hyper-V virtualized environment, install integration services on both the host and guest operating system and configure disks for data volumes with the synthetic SCSI controller.</span></span> <span data-ttu-id="b886e-129">对于跨多个数据驱动器的高密集型存储 I/O 工作负荷，应将每个 VHD 附加到更好的总体性能的单独合成 SCSI 控制器。</span><span class="sxs-lookup"><span data-stu-id="b886e-129">For highly intensive storage I/O workloads that span multiple data drives, each VHD should be attached to a separate synthetic SCSI controller for better overall performance.</span></span> <span data-ttu-id="b886e-130">此外，每个 VHD 应存储在单独的物理磁盘或 Lun 上。</span><span class="sxs-lookup"><span data-stu-id="b886e-130">In addition, each VHD should be stored on separate physical disks or LUNs.</span></span>  
  
#### <a name="measuring-passthrough-disk-performance"></a><span data-ttu-id="b886e-131">测量的传递磁盘性能</span><span class="sxs-lookup"><span data-stu-id="b886e-131">Measuring PassThrough Disk Performance</span></span>  
 <span data-ttu-id="b886e-132">在任何合并练习很重要，若要最大利用可用资源。</span><span class="sxs-lookup"><span data-stu-id="b886e-132">During any consolidation exercise it is important to make maximum use of available resources.</span></span> <span data-ttu-id="b886e-133">如前文所述，SQL 数据卷上的存储 I/O 在 BizTalk Server 解决方案的总体性能中扮演的重要部分。</span><span class="sxs-lookup"><span data-stu-id="b886e-133">As discussed previously, storage I/O on SQL data volumes plays a significant part in the overall performance of a BizTalk Server solution.</span></span> <span data-ttu-id="b886e-134">因此作为本指南的一部分进行了测试对性能的 HYPER-V 中的传递磁盘的物理磁盘的相对性能。</span><span class="sxs-lookup"><span data-stu-id="b886e-134">Therefore as part of this guidance, the relative performance of a physical disk to the performance of a passthrough disk in Hyper-V was tested.</span></span> <span data-ttu-id="b886e-135">MessageBox 数据的相对性能中 Physical_SQL01 的驱动器和 Virtual_SQL01 测量使用开放源代码工具最初由 Intel Corporation 开发和现在维护 IOMeter 通过开放源开发实验室 (OSDL)。</span><span class="sxs-lookup"><span data-stu-id="b886e-135">The relative performance of the MessageBox data drive in Physical_SQL01 and Virtual_SQL01 was measured using the IOMeter open source tool originally developed by Intel Corporation and now maintained by the open Source Development Lab (OSDL).</span></span> <span data-ttu-id="b886e-136">有关 IOMeter 的详细信息，请参阅[ http://go.microsoft.com/fwlink/?LinkId=122412 ](http://go.microsoft.com/fwlink/?LinkId=122412)。</span><span class="sxs-lookup"><span data-stu-id="b886e-136">For more information about IOMeter, see [http://go.microsoft.com/fwlink/?LinkId=122412](http://go.microsoft.com/fwlink/?LinkId=122412).</span></span>  
  
 <span data-ttu-id="b886e-137">下表描述了在测试环境、 已使用的 IOMeter 配置选项、 已运行的测试的说明和结果的摘要中使用的物理和虚拟硬件配置。</span><span class="sxs-lookup"><span data-stu-id="b886e-137">The following tables describe the physical and virtual hardware configuration used in the test environment, the IOMeter configuration options that were used, a description of the test that was run, and a summary of results.</span></span>  
  
#### <a name="configuration-used-for-testing"></a><span data-ttu-id="b886e-138">用于测试的配置</span><span class="sxs-lookup"><span data-stu-id="b886e-138">Configuration Used for Testing</span></span>  
  
### <a name="physicalsql01"></a><span data-ttu-id="b886e-139">Physical_SQL01</span><span class="sxs-lookup"><span data-stu-id="b886e-139">Physical_SQL01</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="b886e-140">**Model**</span><span class="sxs-lookup"><span data-stu-id="b886e-140">**Model**</span></span>|<span data-ttu-id="b886e-141">HP DL580</span><span class="sxs-lookup"><span data-stu-id="b886e-141">HP DL580</span></span>|  
|<span data-ttu-id="b886e-142">**Processor**</span><span class="sxs-lookup"><span data-stu-id="b886e-142">**Processor**</span></span>|<span data-ttu-id="b886e-143">四核处理器，双核 Intel Xeon 2.4 Ghz</span><span class="sxs-lookup"><span data-stu-id="b886e-143">Quad processor, Quad-core Intel Xeon 2.4Ghz</span></span>|  
|<span data-ttu-id="b886e-144">**内存**</span><span class="sxs-lookup"><span data-stu-id="b886e-144">**Memory**</span></span>|<span data-ttu-id="b886e-145">8 GB</span><span class="sxs-lookup"><span data-stu-id="b886e-145">8 GB</span></span>|  
|<span data-ttu-id="b886e-146">**网络**</span><span class="sxs-lookup"><span data-stu-id="b886e-146">**Networking**</span></span>|<span data-ttu-id="b886e-147">HP NC3T3i 多功能千兆 Server 适配器</span><span class="sxs-lookup"><span data-stu-id="b886e-147">HP NC3T3i Multifunction Gigabit Server adapter</span></span>|  
|<span data-ttu-id="b886e-148">**SAN 配置**</span><span class="sxs-lookup"><span data-stu-id="b886e-148">**SAN configuration**</span></span>|<span data-ttu-id="b886e-149">直连的 SAN 存储 （请参见下表）</span><span class="sxs-lookup"><span data-stu-id="b886e-149">Direct attached SAN storage (see table below)</span></span>|  
  
### <a name="physicalsql01--san-configuration"></a><span data-ttu-id="b886e-150">Physical_SQL01 – SAN 配置</span><span class="sxs-lookup"><span data-stu-id="b886e-150">Physical_SQL01 – SAN Configuration</span></span>  
  
|<span data-ttu-id="b886e-151">驱动器号</span><span class="sxs-lookup"><span data-stu-id="b886e-151">Drive letter</span></span>|<span data-ttu-id="b886e-152">Description</span><span class="sxs-lookup"><span data-stu-id="b886e-152">Description</span></span>|<span data-ttu-id="b886e-153">LUN 的大小</span><span class="sxs-lookup"><span data-stu-id="b886e-153">LUN Size</span></span>|<span data-ttu-id="b886e-154">RAID 配置</span><span class="sxs-lookup"><span data-stu-id="b886e-154">RAID configuration</span></span>|  
|------------------|-----------------|--------------|------------------------|  
|<span data-ttu-id="b886e-155">G:</span><span class="sxs-lookup"><span data-stu-id="b886e-155">G:</span></span>|<span data-ttu-id="b886e-156">Data_Sys</span><span class="sxs-lookup"><span data-stu-id="b886e-156">Data_Sys</span></span>|<span data-ttu-id="b886e-157">10</span><span class="sxs-lookup"><span data-stu-id="b886e-157">10</span></span>|<span data-ttu-id="b886e-158">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-158">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-159">H:</span><span class="sxs-lookup"><span data-stu-id="b886e-159">H:</span></span>|<span data-ttu-id="b886e-160">Logs_Sys</span><span class="sxs-lookup"><span data-stu-id="b886e-160">Logs_Sys</span></span>|<span data-ttu-id="b886e-161">10</span><span class="sxs-lookup"><span data-stu-id="b886e-161">10</span></span>|<span data-ttu-id="b886e-162">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-162">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-163">I:</span><span class="sxs-lookup"><span data-stu-id="b886e-163">I:</span></span>|<span data-ttu-id="b886e-164">Data_TempDb</span><span class="sxs-lookup"><span data-stu-id="b886e-164">Data_TempDb</span></span>|<span data-ttu-id="b886e-165">50</span><span class="sxs-lookup"><span data-stu-id="b886e-165">50</span></span>|<span data-ttu-id="b886e-166">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-166">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-167">J:</span><span class="sxs-lookup"><span data-stu-id="b886e-167">J:</span></span>|<span data-ttu-id="b886e-168">Logs_TempDb</span><span class="sxs-lookup"><span data-stu-id="b886e-168">Logs_TempDb</span></span>|<span data-ttu-id="b886e-169">50</span><span class="sxs-lookup"><span data-stu-id="b886e-169">50</span></span>|<span data-ttu-id="b886e-170">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-170">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-171">K:</span><span class="sxs-lookup"><span data-stu-id="b886e-171">K:</span></span>|<span data-ttu-id="b886e-172">Data_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="b886e-172">Data_BtsMsgBox</span></span>|<span data-ttu-id="b886e-173">300</span><span class="sxs-lookup"><span data-stu-id="b886e-173">300</span></span>|<span data-ttu-id="b886e-174">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-174">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-175">L:</span><span class="sxs-lookup"><span data-stu-id="b886e-175">L:</span></span>|<span data-ttu-id="b886e-176">Logs_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="b886e-176">Logs_BtsMsgBox</span></span>|<span data-ttu-id="b886e-177">100</span><span class="sxs-lookup"><span data-stu-id="b886e-177">100</span></span>|<span data-ttu-id="b886e-178">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-178">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-179">M:</span><span class="sxs-lookup"><span data-stu-id="b886e-179">M:</span></span>|<span data-ttu-id="b886e-180">MSDTC</span><span class="sxs-lookup"><span data-stu-id="b886e-180">MSDTC</span></span>|<span data-ttu-id="b886e-181">5</span><span class="sxs-lookup"><span data-stu-id="b886e-181">5</span></span>|<span data-ttu-id="b886e-182">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-182">RAID 0 + 1</span></span>|  
  
### <a name="hyper-vhostsql01"></a><span data-ttu-id="b886e-183">Hyper-V_Host_SQL01</span><span class="sxs-lookup"><span data-stu-id="b886e-183">Hyper-V_Host_SQL01</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="b886e-184">**Model**</span><span class="sxs-lookup"><span data-stu-id="b886e-184">**Model**</span></span>|<span data-ttu-id="b886e-185">HP DL580</span><span class="sxs-lookup"><span data-stu-id="b886e-185">HP DL580</span></span>|  
|<span data-ttu-id="b886e-186">**Processor**</span><span class="sxs-lookup"><span data-stu-id="b886e-186">**Processor**</span></span>|<span data-ttu-id="b886e-187">四核处理器，双核 Intel Xeon 2.4 Ghz</span><span class="sxs-lookup"><span data-stu-id="b886e-187">Quad processor, Quad-core Intel Xeon 2.4Ghz</span></span>|  
|<span data-ttu-id="b886e-188">**内存**</span><span class="sxs-lookup"><span data-stu-id="b886e-188">**Memory**</span></span>|<span data-ttu-id="b886e-189">32 GB</span><span class="sxs-lookup"><span data-stu-id="b886e-189">32 GB</span></span>|  
|<span data-ttu-id="b886e-190">**网络**</span><span class="sxs-lookup"><span data-stu-id="b886e-190">**Networking**</span></span>|<span data-ttu-id="b886e-191">Broadcom BCM5708C NetXtreme II GigEHP DL380 G5</span><span class="sxs-lookup"><span data-stu-id="b886e-191">Broadcom BCM5708C NetXtreme II GigEHP DL380 G5</span></span>|  
  
### <a name="virtualsql01---virtual-machine-configuration"></a><span data-ttu-id="b886e-192">Virtual_SQL01-虚拟机配置</span><span class="sxs-lookup"><span data-stu-id="b886e-192">Virtual_SQL01 - Virtual Machine Configuration</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="b886e-193">**虚拟处理器**</span><span class="sxs-lookup"><span data-stu-id="b886e-193">**Virtual processors**</span></span>|<span data-ttu-id="b886e-194">分配的 4</span><span class="sxs-lookup"><span data-stu-id="b886e-194">4 allocated</span></span>|  
|<span data-ttu-id="b886e-195">**内存**</span><span class="sxs-lookup"><span data-stu-id="b886e-195">**Memory**</span></span>|<span data-ttu-id="b886e-196">8 GB</span><span class="sxs-lookup"><span data-stu-id="b886e-196">8 GB</span></span>|  
|<span data-ttu-id="b886e-197">**网络**</span><span class="sxs-lookup"><span data-stu-id="b886e-197">**Networking**</span></span>|<span data-ttu-id="b886e-198">虚拟机网络连接到：</span><span class="sxs-lookup"><span data-stu-id="b886e-198">Virtual Machine Networking connected to:</span></span><br /><span data-ttu-id="b886e-199">Broadcom BCM5708C NetXtreme II GigE</span><span class="sxs-lookup"><span data-stu-id="b886e-199">Broadcom BCM5708C NetXtreme II GigE</span></span>|  
|<span data-ttu-id="b886e-200">**硬盘配置**</span><span class="sxs-lookup"><span data-stu-id="b886e-200">**Hard disk configuration**</span></span>|<span data-ttu-id="b886e-201">**IDE 控制器**– 30 GB 的操作系统为固定 vhd</span><span class="sxs-lookup"><span data-stu-id="b886e-201">**IDE controller** – 30 GB fixed vhd for Operating System</span></span><br /><span data-ttu-id="b886e-202">**SCSI 控制器**-7 直接连接 （请参见下表） 的传递 SAN Lun</span><span class="sxs-lookup"><span data-stu-id="b886e-202">**SCSI controller** - 7 directly attached passthrough SAN LUNs (see table below)</span></span>|  
  
### <a name="virtualsql01--san-configuration"></a><span data-ttu-id="b886e-203">Virtual_SQL01 – SAN Configuration</span><span class="sxs-lookup"><span data-stu-id="b886e-203">Virtual_SQL01 – SAN Configuration</span></span>  
  
|<span data-ttu-id="b886e-204">驱动器号</span><span class="sxs-lookup"><span data-stu-id="b886e-204">Drive letter</span></span>|<span data-ttu-id="b886e-205">Description</span><span class="sxs-lookup"><span data-stu-id="b886e-205">Description</span></span>|<span data-ttu-id="b886e-206">LUN 的大小</span><span class="sxs-lookup"><span data-stu-id="b886e-206">LUN Size</span></span>|<span data-ttu-id="b886e-207">RAID 配置</span><span class="sxs-lookup"><span data-stu-id="b886e-207">RAID configuration</span></span>|  
|------------------|-----------------|--------------|------------------------|  
|<span data-ttu-id="b886e-208">G:</span><span class="sxs-lookup"><span data-stu-id="b886e-208">G:</span></span>|<span data-ttu-id="b886e-209">Data_Sys</span><span class="sxs-lookup"><span data-stu-id="b886e-209">Data_Sys</span></span>|<span data-ttu-id="b886e-210">10</span><span class="sxs-lookup"><span data-stu-id="b886e-210">10</span></span>|<span data-ttu-id="b886e-211">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-211">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-212">H:</span><span class="sxs-lookup"><span data-stu-id="b886e-212">H:</span></span>|<span data-ttu-id="b886e-213">Logs_Sys</span><span class="sxs-lookup"><span data-stu-id="b886e-213">Logs_Sys</span></span>|<span data-ttu-id="b886e-214">10</span><span class="sxs-lookup"><span data-stu-id="b886e-214">10</span></span>|<span data-ttu-id="b886e-215">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-215">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-216">I:</span><span class="sxs-lookup"><span data-stu-id="b886e-216">I:</span></span>|<span data-ttu-id="b886e-217">Data_TempDb</span><span class="sxs-lookup"><span data-stu-id="b886e-217">Data_TempDb</span></span>|<span data-ttu-id="b886e-218">50</span><span class="sxs-lookup"><span data-stu-id="b886e-218">50</span></span>|<span data-ttu-id="b886e-219">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-219">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-220">J:</span><span class="sxs-lookup"><span data-stu-id="b886e-220">J:</span></span>|<span data-ttu-id="b886e-221">Logs_TempDb</span><span class="sxs-lookup"><span data-stu-id="b886e-221">Logs_TempDb</span></span>|<span data-ttu-id="b886e-222">50</span><span class="sxs-lookup"><span data-stu-id="b886e-222">50</span></span>|<span data-ttu-id="b886e-223">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-223">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-224">K:</span><span class="sxs-lookup"><span data-stu-id="b886e-224">K:</span></span>|<span data-ttu-id="b886e-225">Data_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="b886e-225">Data_BtsMsgBox</span></span>|<span data-ttu-id="b886e-226">300</span><span class="sxs-lookup"><span data-stu-id="b886e-226">300</span></span>|<span data-ttu-id="b886e-227">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-227">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-228">L:</span><span class="sxs-lookup"><span data-stu-id="b886e-228">L:</span></span>|<span data-ttu-id="b886e-229">Logs_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="b886e-229">Logs_BtsMsgBox</span></span>|<span data-ttu-id="b886e-230">100</span><span class="sxs-lookup"><span data-stu-id="b886e-230">100</span></span>|<span data-ttu-id="b886e-231">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-231">RAID 0 + 1</span></span>|  
|<span data-ttu-id="b886e-232">M:</span><span class="sxs-lookup"><span data-stu-id="b886e-232">M:</span></span>|<span data-ttu-id="b886e-233">MSDTC</span><span class="sxs-lookup"><span data-stu-id="b886e-233">MSDTC</span></span>|<span data-ttu-id="b886e-234">5</span><span class="sxs-lookup"><span data-stu-id="b886e-234">5</span></span>|<span data-ttu-id="b886e-235">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="b886e-235">RAID 0 + 1</span></span>|  
  
#### <a name="iometer-configuration"></a><span data-ttu-id="b886e-236">IOMeter 配置</span><span class="sxs-lookup"><span data-stu-id="b886e-236">IOMeter Configuration</span></span>  
 <span data-ttu-id="b886e-237">IOMeter 工具可以用作基准和故障排除工具通过复制应用程序的读/写性能。</span><span class="sxs-lookup"><span data-stu-id="b886e-237">The IOMeter tool can be used as a benchmark and troubleshooting tool by replicating the read/write performance of applications.</span></span> <span data-ttu-id="b886e-238">IOMeter 是性能的一个可配置的工具，可用于模拟许多不同类型。</span><span class="sxs-lookup"><span data-stu-id="b886e-238">IOMeter is a configurable tool that can be used to simulate many different types of performance.</span></span> <span data-ttu-id="b886e-239">对于此测试方案，IOMeter 配置参数被设置为在下表描述这两个物理[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]已测试的计算机和运行来宾操作系统上[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]在 HYPER-V 虚拟机:</span><span class="sxs-lookup"><span data-stu-id="b886e-239">For purposes of this test scenario, IOMeter configuration parameters were set as described in the table below on both the physical [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] computer that was tested and on the guest operating system that was running [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] in a Hyper-V virtual machine:</span></span>  
  
### <a name="iometer--passthrough-disk-comparison-test-configuration"></a><span data-ttu-id="b886e-240">IOMeter – 传递磁盘比较测试配置</span><span class="sxs-lookup"><span data-stu-id="b886e-240">IOMeter – Passthrough Disk Comparison Test Configuration</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="b886e-241">**测试长度**</span><span class="sxs-lookup"><span data-stu-id="b886e-241">**Test length**</span></span>|<span data-ttu-id="b886e-242">10 分钟。</span><span class="sxs-lookup"><span data-stu-id="b886e-242">10 minutes</span></span>|  
|<span data-ttu-id="b886e-243">**加速时间**</span><span class="sxs-lookup"><span data-stu-id="b886e-243">**Ramp up time**</span></span>|<span data-ttu-id="b886e-244">30 秒</span><span class="sxs-lookup"><span data-stu-id="b886e-244">30 seconds</span></span>|  
|<span data-ttu-id="b886e-245">**辅助进程数**</span><span class="sxs-lookup"><span data-stu-id="b886e-245">**Number of workers**</span></span>|<span data-ttu-id="b886e-246">4</span><span class="sxs-lookup"><span data-stu-id="b886e-246">4</span></span>|  
|<span data-ttu-id="b886e-247">**传输请求大小**</span><span class="sxs-lookup"><span data-stu-id="b886e-247">**Transfer request size**</span></span>|<span data-ttu-id="b886e-248">2 KB</span><span class="sxs-lookup"><span data-stu-id="b886e-248">2 KB</span></span>|  
|<span data-ttu-id="b886e-249">**读/写分发**</span><span class="sxs-lookup"><span data-stu-id="b886e-249">**Read/write distribution**</span></span>|<span data-ttu-id="b886e-250">66%读取，33%写入</span><span class="sxs-lookup"><span data-stu-id="b886e-250">66% read, 33% write</span></span>|  
|<span data-ttu-id="b886e-251">**迸发长度**</span><span class="sxs-lookup"><span data-stu-id="b886e-251">**Burst length**</span></span>|<span data-ttu-id="b886e-252">1 I/o</span><span class="sxs-lookup"><span data-stu-id="b886e-252">1 I/Os</span></span>|  
|<span data-ttu-id="b886e-253">**目标驱动器**</span><span class="sxs-lookup"><span data-stu-id="b886e-253">**Target Drive**</span></span>|<span data-ttu-id="b886e-254">K:\\</span><span class="sxs-lookup"><span data-stu-id="b886e-254">K:\\</span></span>|  
  
#### <a name="test-description"></a><span data-ttu-id="b886e-255">测试说明</span><span class="sxs-lookup"><span data-stu-id="b886e-255">Test Description</span></span>  
 <span data-ttu-id="b886e-256">[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]服务已停止两个服务器以确保 IOMeter 是仅对磁盘执行 I/O 的进程上。</span><span class="sxs-lookup"><span data-stu-id="b886e-256">The [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] service was stopped on both servers to ensure that IOMeter was the only process performing I/O against the disk.</span></span> <span data-ttu-id="b886e-257">LUN 的使用在此测试中都位于相同的专用于此实验室环境的 SAN 上。</span><span class="sxs-lookup"><span data-stu-id="b886e-257">The LUN’s used in this test were both located on the same SAN which was dedicated to this lab environment.</span></span> <span data-ttu-id="b886e-258">针对 SAN 上以确保结果的不倾斜在测试期间不执行任何其他 I/O 活动。</span><span class="sxs-lookup"><span data-stu-id="b886e-258">No other I/O activity was performed against the SAN during the test to ensure that the results were not skewed.</span></span> <span data-ttu-id="b886e-259">通过从每个本地执行 IOMeter 工具然后运行测试[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]和收集以下性能监视器计数器：</span><span class="sxs-lookup"><span data-stu-id="b886e-259">The test was then run by executing the IOMeter tool locally from each [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] and the following performance monitor counters were collected:</span></span>  
  
 <span data-ttu-id="b886e-260">**收集从 Virtual_SQL01 和 Physical_SQL01**:</span><span class="sxs-lookup"><span data-stu-id="b886e-260">**Collected from both Virtual_SQL01 and Physical_SQL01**:</span></span>  
  
-   <span data-ttu-id="b886e-261">\LogicalDisk(*)\\\*</span><span class="sxs-lookup"><span data-stu-id="b886e-261">\LogicalDisk(*)\\\*</span></span>  
  
-   <span data-ttu-id="b886e-262">\PhysicalDisk(*)\\\*</span><span class="sxs-lookup"><span data-stu-id="b886e-262">\PhysicalDisk(*)\\\*</span></span>  
  
 <span data-ttu-id="b886e-263">**收集从虚拟机超 V_02**:</span><span class="sxs-lookup"><span data-stu-id="b886e-263">**Collected from virtual machine Hyper-V_02**:</span></span>  
  
-   <span data-ttu-id="b886e-264">\Hyper-V 虚拟存储设备\\*</span><span class="sxs-lookup"><span data-stu-id="b886e-264">\Hyper-V Virtual Storage Device\\*</span></span>  
  
### <a name="results"></a><span data-ttu-id="b886e-265">结果</span><span class="sxs-lookup"><span data-stu-id="b886e-265">Results</span></span>  
 <span data-ttu-id="b886e-266">传递磁盘时能够获得超过 90%的直接连接到 Physical_SQL01 的 SAN LUN 的吞吐量。</span><span class="sxs-lookup"><span data-stu-id="b886e-266">The passthrough disk was able to attain over 90% of the throughput of the SAN LUN connected directly to Physical_SQL01.</span></span>  <span data-ttu-id="b886e-267">总，读取和写入相同的每秒传输的总 MB，每秒 I/o 已都处于 10%。</span><span class="sxs-lookup"><span data-stu-id="b886e-267">Total, read and write I/Os per second were all within 10% as was the total MB transferred per second.</span></span>  <span data-ttu-id="b886e-268">正常的磁盘的响应时间应介于 1-15 毫秒的时间让读取和写入。</span><span class="sxs-lookup"><span data-stu-id="b886e-268">Response times for healthy disks should be between 1-15 ms for read and write.</span></span> <span data-ttu-id="b886e-269">平均 I/O 响应时间是小于 4 毫秒，这两个磁盘上。</span><span class="sxs-lookup"><span data-stu-id="b886e-269">Average I/O response times were less than 4 ms on both disks.</span></span> <span data-ttu-id="b886e-270">随机读取响应时间为 5.4 ms 在物理上和传递磁盘上的 5.7 ms。</span><span class="sxs-lookup"><span data-stu-id="b886e-270">Random reads response time was 5.4 ms on the physical and 5.7 ms on the pass-through disk.</span></span> <span data-ttu-id="b886e-271">写入响应时间是小于 0.5 毫秒上物理和虚拟环境。</span><span class="sxs-lookup"><span data-stu-id="b886e-271">Write response time was less than 0.5 ms on both the physical and virtual environments.</span></span>  
  
 <span data-ttu-id="b886e-272">结果指示使用启用的 SCSI 控制器的传递磁盘，可以提供超过 90%的直接连接的物理磁盘的性能。</span><span class="sxs-lookup"><span data-stu-id="b886e-272">The results indicate that a passthrough disk using the enlightened SCSI controller can provide over 90% of the performance of a directly connected physical disk.</span></span> <span data-ttu-id="b886e-273">I/O 子系统性能至关重要高效 BizTalk Server 操作，通过提供极好的吞吐量和响应时间 HYPER-V 适用的最佳候选项整合 BizTalk Server 环境。</span><span class="sxs-lookup"><span data-stu-id="b886e-273">I/O subsystem performance is critical for efficient BizTalk Server operation, by providing excellent throughput and response times Hyper-V is an excellent candidate for consolidating a BizTalk Server environment.</span></span> <span data-ttu-id="b886e-274">下表提供了到物理磁盘的传递磁盘的性能进行比较时，观察到的磁盘测试结果摘要：</span><span class="sxs-lookup"><span data-stu-id="b886e-274">The table below provides a summary of the disk test results observed when comparing performance of a passthrough disk to a physical disk:</span></span>  
  
|<span data-ttu-id="b886e-275">度量</span><span class="sxs-lookup"><span data-stu-id="b886e-275">Measurement</span></span>|<span data-ttu-id="b886e-276">Physical_SQL01 （物理磁盘）</span><span class="sxs-lookup"><span data-stu-id="b886e-276">Physical_SQL01 (Physical Disk)</span></span>|<span data-ttu-id="b886e-277">Virtual_SQL01 (passthrough)</span><span class="sxs-lookup"><span data-stu-id="b886e-277">Virtual_SQL01 (passthrough)</span></span>|<span data-ttu-id="b886e-278">传递磁盘的物理磁盘的相对性能</span><span class="sxs-lookup"><span data-stu-id="b886e-278">Relative performance of passthrough disks to physical disks</span></span>|  
|-----------------|---------------------------------------|------------------------------------|-----------------------------------------------------------------|  
|<span data-ttu-id="b886e-279">每秒的总 I/o</span><span class="sxs-lookup"><span data-stu-id="b886e-279">Total I/Os per second</span></span>|<span data-ttu-id="b886e-280">269.73</span><span class="sxs-lookup"><span data-stu-id="b886e-280">269.73</span></span>|<span data-ttu-id="b886e-281">250.47</span><span class="sxs-lookup"><span data-stu-id="b886e-281">250.47</span></span>|<span data-ttu-id="b886e-282">92.86%</span><span class="sxs-lookup"><span data-stu-id="b886e-282">92.86%</span></span>|  
|<span data-ttu-id="b886e-283">每秒读取 I/o</span><span class="sxs-lookup"><span data-stu-id="b886e-283">Read I/Os per second</span></span>|<span data-ttu-id="b886e-284">180.73</span><span class="sxs-lookup"><span data-stu-id="b886e-284">180.73</span></span>|<span data-ttu-id="b886e-285">167.60</span><span class="sxs-lookup"><span data-stu-id="b886e-285">167.60</span></span>|<span data-ttu-id="b886e-286">92.74%</span><span class="sxs-lookup"><span data-stu-id="b886e-286">92.74%</span></span>|  
|<span data-ttu-id="b886e-287">每秒写入 I/o</span><span class="sxs-lookup"><span data-stu-id="b886e-287">Write I/Os per second</span></span>|<span data-ttu-id="b886e-288">89.00</span><span class="sxs-lookup"><span data-stu-id="b886e-288">89.00</span></span>|<span data-ttu-id="b886e-289">82.87</span><span class="sxs-lookup"><span data-stu-id="b886e-289">82.87</span></span>|<span data-ttu-id="b886e-290">93.11%</span><span class="sxs-lookup"><span data-stu-id="b886e-290">93.11%</span></span>|  
|<span data-ttu-id="b886e-291">总 Mb / 秒</span><span class="sxs-lookup"><span data-stu-id="b886e-291">Total MBs per second</span></span>|<span data-ttu-id="b886e-292">0.53</span><span class="sxs-lookup"><span data-stu-id="b886e-292">0.53</span></span>|<span data-ttu-id="b886e-293">0.49</span><span class="sxs-lookup"><span data-stu-id="b886e-293">0.49</span></span>|<span data-ttu-id="b886e-294">92.45%</span><span class="sxs-lookup"><span data-stu-id="b886e-294">92.45%</span></span>|  
|<span data-ttu-id="b886e-295">平均读取响应时间 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="b886e-295">Average read response time (ms)</span></span>|<span data-ttu-id="b886e-296">5.4066</span><span class="sxs-lookup"><span data-stu-id="b886e-296">5.4066</span></span>|<span data-ttu-id="b886e-297">5.7797</span><span class="sxs-lookup"><span data-stu-id="b886e-297">5.7797</span></span>|<span data-ttu-id="b886e-298">93.54%</span><span class="sxs-lookup"><span data-stu-id="b886e-298">93.54%</span></span>|  
|<span data-ttu-id="b886e-299">平均写入响应时间 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="b886e-299">Average write response time (ms)</span></span>|<span data-ttu-id="b886e-300">0.2544</span><span class="sxs-lookup"><span data-stu-id="b886e-300">0.2544</span></span>|<span data-ttu-id="b886e-301">0.3716</span><span class="sxs-lookup"><span data-stu-id="b886e-301">0.3716</span></span>|<span data-ttu-id="b886e-302">68.42%**注意：**虽然的传递磁盘平均写入响应时间的相对性能已 68.42%的物理磁盘的性能，传递磁盘的平均写入响应时间为仍很好地内建立可接受的限制的 10 毫秒。</span><span class="sxs-lookup"><span data-stu-id="b886e-302">68.42% **Note:**  Although the relative performance of the pass through disks for Average write response time was 68.42% of the performance of physical disks, the Average write response time of the passthrough disks was still well within established acceptable limits of 10 ms.</span></span>|  
|<span data-ttu-id="b886e-303">平均 I/O 响应时间 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="b886e-303">Average I/O response time (ms)</span></span>|<span data-ttu-id="b886e-304">3.7066</span><span class="sxs-lookup"><span data-stu-id="b886e-304">3.7066</span></span>|<span data-ttu-id="b886e-305">3.9904</span><span class="sxs-lookup"><span data-stu-id="b886e-305">3.9904</span></span>|<span data-ttu-id="b886e-306">93.89%</span><span class="sxs-lookup"><span data-stu-id="b886e-306">93.89%</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="b886e-307">每秒的总 I/o、 每秒读取 I/o、 写入每秒的 i/o 数和总 Mb / 秒的百分比值除以传递磁盘值的相应的物理磁盘值计算。</span><span class="sxs-lookup"><span data-stu-id="b886e-307">The percentage values for Total I/Os per second, Read I/Os per second, Write I/Os per second, and Total MBs per second were calculated by dividing passthrough disk values by the corresponding physical disk values.</span></span>  
>   
>  <span data-ttu-id="b886e-308">平均值的百分比值读取响应时间 （毫秒），平均写入响应时间 （毫秒），并除以物理磁盘的相应的传递磁盘值的值计算平均 I/O 响应时间 （毫秒）。</span><span class="sxs-lookup"><span data-stu-id="b886e-308">The percentage values for Average read response time (ms), Average write response time (ms), and Average I/O response time (ms) were calculated by dividing physical disk values by the corresponding passthrough disk values.</span></span>